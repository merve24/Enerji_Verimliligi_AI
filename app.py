import streamlit as st
from google import genai
import numpy as np
import os
from data import prepare_rag_data 

# --- Sayfa KonfigÃ¼rasyonu (YENÄ°) ---
st.set_page_config(
    page_title="Enerji VerimliliÄŸi RAG UzmanÄ±",
    page_icon="ğŸ’¡",
    layout="wide"
)

# --- 1. SABÄ°T VERÄ°LERÄ° VE BAÄLANTILARI TANIMLAMA ---
try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"] 
except Exception as e:
    st.error(f"API AnahtarÄ± YÃ¼klenemedi: LÃ¼tfen Streamlit Cloud Secrets'ta GEMINI_API_KEY'i tanÄ±mladÄ±ÄŸÄ±nÄ±zdan emin olun. Hata: {e}")
    st.stop()
    
# Cache kullanarak verinin sadece BÄ°R KEZ yÃ¼klenmesini saÄŸlÄ±yoruz
@st.cache_resource(ttl=900) 
def load_rag_data(api_key):
    return prepare_rag_data(api_key)

# Veri yÃ¼kleme fonksiyonunu API key ile Ã§aÄŸÄ±rÄ±yoruz.
text_chunks, index = load_rag_data(GEMINI_API_KEY) 

# EÄŸer data.py hata yakalayÄ±p boÅŸ dÃ¶ndÃ¼rÃ¼rse, burada uygulamayÄ± durdururuz.
if index is None:
    file_name = "Enerji_verimliligi_eÄŸitim_kitabi.txt" 
    st.error(f"Veri seti yÃ¼klenemedi. LÃ¼tfen '{file_name}' dosyasÄ±nÄ±n GitHub'da bulunduÄŸundan emin olun.")
    st.stop()

# Client'Ä± sadece Gemini modelini Ã§aÄŸÄ±rmak iÃ§in bir kez tanÄ±mlÄ±yoruz.
client = genai.Client(api_key=GEMINI_API_KEY) 


# --- 2. RAG Sorgu Fonksiyonu ---
def rag_query_streamlit(query, k=5):
    
    # 2A. Sorguyu VektÃ¶rleÅŸtirme (Retrieval)
    query_embedding = client.models.embed_content(
        model='text-embedding-004',
        contents=[query]
    ).embeddings[0].values
    
    # 2B. FAISS'te en yakÄ±n parÃ§alarÄ± arama
    D, I = index.search(np.array([query_embedding], dtype='float32'), k)
    
    # 2C. ParÃ§alarÄ± metin olarak birleÅŸtirme
    retrieved_text = "\n---\n".join([text_chunks[i] for i in I[0]])
    
    # 2D. Prompt oluÅŸturma (System Prompt ile)
    prompt = f"""
    Sen, "Enerji VerimliliÄŸi EÄŸitim KitabÄ±"ndan bilgi alan uzman bir danÄ±ÅŸmansÄ±n.
    GÃ¶revin, Ã¶ncelikli olarak **aÅŸaÄŸÄ±daki BaÄŸlam (Context)** iÃ§inde yer alan bilgilere dayanarak cevap vermektir.
    
    Talimatlar:
    1. Kitaptaki bilgiyi anlaÅŸÄ±lÄ±r, akÄ±cÄ± ve profesyonel bir dille **Ã¶zetle ve yorumla**. Asla metni olduÄŸu gibi kopyalama.
    2. CevabÄ±n, kitaptan gelen bilgiyle tutarlÄ± olmalÄ±dÄ±r.
    3. **Ã‡ok Ã¶nemli:** EÄŸer kitaptan gelen BaÄŸlam (Context) cevabÄ± desteklemiyorsa, kendi **genel bilgini ve mantÄ±ÄŸÄ±nÄ±** kullanarak en iyi tahmini veya bilgiyi sun, ancak cevabÄ±na ÅŸunu ekle: "(Kitapta bu konuyla ilgili net bir bilgi bulunmamaktadÄ±r, bu genel bir bilgidir.)"
    4. CevabÄ±nÄ± Soruya Ã¶zel olarak kiÅŸiselleÅŸtir.

    ---
    
    BaÄŸlam:
    {retrieved_text}

    ---
    
    Soru:
    {query}
    """
    
    # 2E. Cevap Ãœretme (Generation)
    response = client.models.generate_content(
        model='gemini-2.5-flash',
        contents=prompt
    )
    
    return response.text, retrieved_text

# --- 3. UYGULAMA MANTIÄI VE ARAYÃœZ ---
st.title("ğŸ’¡ Enerji VerimliliÄŸi RAG UzmanÄ±") 
st.markdown("Merhaba! Bu uzman chatbot, 'Enerji VerimliliÄŸi EÄŸitim KitabÄ±'ndan alÄ±nan **gÃ¼ncel bilgilere** ğŸ“š dayanarak sorularÄ±nÄ±zÄ± yanÄ±tlar. **Enerji tasarrufu** ğŸŒ ve **verimlilik** konusunda hemen soru sormaya baÅŸlayÄ±n! ğŸ‘‡") # EMOJÄ°LER EKLENDÄ°

if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "Merhaba! Enerji verimliliÄŸi hakkÄ±nda sorularÄ±nÄ±zÄ± yanÄ±tlamak iÃ§in buradayÄ±m."}]

for msg in st.session_state.messages:
    # Emoji eklendi
    avatar = "ğŸ’¡" if msg["role"] == "assistant" else "ğŸ‘¤" 
    st.chat_message(msg["role"], avatar=avatar).write(msg["content"])

if prompt := st.chat_input("Enerji tasarrufu ipuÃ§larÄ± mÄ± arÄ±yorsunuz? ğŸ” Bir soru sorun..."): # SORU KUTUSUNA EMOJÄ° EKLENDÄ°
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user", avatar="ğŸ‘¤").write(prompt)

    with st.chat_message("assistant", avatar="ğŸ’¡"):
        with st.spinner("Enerji VerimliliÄŸi KitabÄ± taranÄ±yor... â³"): # SPINNER'A EMOJÄ° EKLENDÄ°
            response, source = rag_query_streamlit(prompt)

        st.markdown(response)

        with st.expander("KullanÄ±lan Kaynak (RAG Retrieval) ğŸ“–"): # EXPANDER BAÅLIÄINA EMOJÄ° EKLENDÄ°
            st.code(source, language='text')

    st.session_state.messages.append({"role": "assistant", "content": response})
