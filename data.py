import os
import streamlit as st
from google import genai
import math
import faiss
import numpy as np

# --- 1. COSINE SIMILARITY ---
def cosine_similarity(vec1, vec2):
    dot_product = sum(v1 * v2 for v1, v2 in zip(vec1, vec2))
    magnitude_v1 = math.sqrt(sum(v**2 for v in vec1))
    magnitude_v2 = math.sqrt(sum(v**2 for v in vec2))
    if magnitude_v1 == 0 or magnitude_v2 == 0:
        return 0
    return dot_product / (magnitude_v1 * magnitude_v2)

# --- 2. RAG VERÄ° HAZIRLAMA ---
@st.cache_resource
def prepare_rag_data(api_key):
    file_path = "Enerji_verimliligi_eÄŸitim_kitabi.txt"

    if not os.path.exists(file_path):
        st.error(f"HATA: Veri dosyasÄ± '{file_path}' bulunamadÄ±.")
        return [], None, None

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
    except Exception as e:
        st.error(f"Dosya okuma hatasÄ±: {e}")
        return [], None, None

    # Daha kÃ¼Ã§Ã¼k chunk boyutu â†’ yanÄ±t kesilmesini Ã¶nler
    chunk_size = 1500
    text_chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

    st.info(f"{len(text_chunks)} metin parÃ§asÄ± iÃ§in embedding oluÅŸturuluyor. LÃ¼tfen bekleyin...")

    embeddings = []
    try:
        client = genai.Client(api_key=api_key)
        progress_bar = st.progress(0, text="Embedding OluÅŸturuluyor...")

        for i, chunk in enumerate(text_chunks):
            response = client.models.embed_content(
                model='text-embedding-004',
                contents=[chunk]
            )
            if hasattr(response, "embeddings"):
                embedding_vector = response.embeddings[0].values
            else:
                embedding_vector = response.embedding
            embeddings.append(embedding_vector)
            progress_bar.progress((i + 1) / len(text_chunks))

        progress_bar.empty()

        # --- FAISS INDEX OLUÅTURMA ---
        dimension = len(embeddings[0])
        index = faiss.IndexFlatL2(dimension)
        index.add(np.array(embeddings).astype('float32'))

        return text_chunks, embeddings, index

    except Exception as e:
        st.error(f"Embedding HatasÄ±: {e}")
        return [], None, None

# --- 3. SORGULAMA FONKSÄ°YONU ---
def simple_query_streamlit(prompt, text_chunks, embeddings, index, api_key):
    try:
        client = genai.Client(api_key=api_key)

        # Prompt embedding
        prompt_for_embed = f"Bu sorunun anlamÄ±: {prompt} (enerji verimliliÄŸi baÄŸlamÄ±nda)"
        prompt_response = client.models.embed_content(
            model='text-embedding-004',
            contents=[prompt_for_embed]
        )

        if hasattr(prompt_response, "embeddings"):
            prompt_embedding = prompt_response.embeddings[0].values
        else:
            prompt_embedding = prompt_response.embedding

        # --- FAISS ile en yakÄ±n chunk'larÄ± bul ---
        query_vector = np.array([prompt_embedding]).astype('float32')
        k = 7  # En yakÄ±n 7 chunk â†’ daha fazla bilgi
        distances, indices = index.search(query_vector, k)
        retrieved_text = "\n\n---\n\n".join([text_chunks[i] for i in indices[0]])
        max_chars = 4000  # Daha uzun context
        retrieved_text = retrieved_text[:max_chars]

    except Exception as e:
        st.warning(f"VektÃ¶r Arama HatasÄ±: {e}. Basit aramaya geÃ§iliyor.")
        retrieved_text = "\n\n---\n\n".join(text_chunks[:3])

    # --- MODEL CEVAP ÃœRETÄ°MÄ° ---
    try:
        rag_prompt = (
            f"Sen 'Enerji VerimliliÄŸi AI Chatbot'u olarak Enerji VerimliliÄŸi EÄŸitim KitabÄ±'na dayalÄ± "
            f"bir yapay zeka asistansÄ±n.\n"
            f"KullanÄ±cÄ± sorularÄ±nÄ± aÅŸaÄŸÄ±daki kaynak metinlere dayanarak mantÄ±klÄ±, detaylÄ± ve kiÅŸiselleÅŸtirilmiÅŸ "
            f"bir ÅŸekilde cevapla.\n"
            f"- Kitaptaki bilgilerden yararlanarak soruya uygun akÄ±l yÃ¼rÃ¼tebilirsin.\n"
            f"- EÄŸer kaynak metinde doÄŸrudan bilgi yoksa, mantÄ±klÄ± Ã§Ä±karÄ±mlar yaparak cevabÄ± oluÅŸturabilirsin, "
            f"ama kitaptaki konulardan Ã§ok sapmamalÄ±sÄ±n.\n"
            f"- Bilgiye tamamen dayalÄ± olmayan veya gerÃ§ek dÄ±ÅŸÄ± (halÃ¼sinasyon) cevaplar Ã¼retme.\n\n"
            f"KAYNAK METÄ°N:\n---\n{retrieved_text}\n---\n\n"
            f"KULLANICI SORUSU: {prompt}\n\n"
            f"CevabÄ± eksiksiz, anlaÅŸÄ±lÄ±r ve mantÄ±ksal olarak tutarlÄ± ÅŸekilde ver."
        )

        response = client.models.generate_content(
            model='gemini-2.5-flash',
            contents=rag_prompt
        )

        # Cevap kÄ±sa ise detaylandÄ±r
        if len(response.text) < 100:
            follow_up_prompt = rag_prompt + "\nLÃ¼tfen cevabÄ± daha detaylÄ± ve mantÄ±klÄ± ÅŸekilde aÃ§Ä±kla."
            response = client.models.generate_content(
                model='gemini-2.5-flash',
                contents=follow_up_prompt
            )

        # ğŸ”§ Kaynak metin artÄ±k dÃ¶ndÃ¼rÃ¼lmÃ¼yor
        return response.text

    except Exception as e:
        st.error(f"Sorgulama HatasÄ±: {e}")
        return "ÃœzgÃ¼nÃ¼m, sorgulama sÄ±rasÄ±nda bir hata oluÅŸtu."
